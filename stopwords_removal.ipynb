{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37c6b441-91ca-4951-bcde-a26bcc25514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['algorithm', 'uses', 'dictionary', 'store', 'numbers', 'iterates', 'list', '.', 'number', ',', 'checks', 'whether', 'complement', '(', 'target', '−', 'current', 'number', ')', 'already', 'exists', '—', ',', 'returns', 'indices', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "text='The algorithm uses a dictionary to store numbers as it iterates through the list. For each number, it checks whether the complement (target − current number) already exists — if so, it returns their indices.'\n",
    "words=word_tokenize(text)\n",
    "stop_words=set(stopwords.words('english'))\n",
    "filtered_words=[w for w in words if w.lower() not in stop_words]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e573d93-31d9-41c0-a466-546c3fcb7cd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3186844948.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    filtered_words=[token.text for token in doc token.is_stop]\u001b[0m\n\u001b[1;37m                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# in sparcy\n",
    "import spacy\n",
    "nlp.spacy.load('en_core_web_sm')\n",
    "text='The algorithm uses a dictionary to store numbers as it iterates through the list. For each number, it checks whether the complement (target − current number) already exists — if so, it returns their indices.'\n",
    "doc=nlp(text)\n",
    "filtered_words=[token.text for token in doc token.is_stop]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1813206e-b033-45f7-b4d5-5b2093461140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['algorithm', 'uses', 'dictionary', 'store', 'numbers', 'iterates', 'list', 'number', 'checks', 'whether', 'complement', 'target', '−', 'current', 'number', 'already', 'exists', '—', 'returns', 'indices']\n"
     ]
    }
   ],
   "source": [
    "#in textblob\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "text='The algorithm uses a dictionary to store numbers as it iterates through the list. For each number, it checks whether the complement (target − current number) already exists — if so, it returns their indices.'\n",
    "blob=TextBlob(text)\n",
    "stop_words=set(stopwords.words('english'))\n",
    "filtered_words=[word for word in blob.words if word.lower() not in stop_words]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db476c3-b055-4f22-8fc9-02db96eb2059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm us dictionary store number iterates list number check whether complement target current number already exists return index\n"
     ]
    }
   ],
   "source": [
    "#full cleaning \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "text='The algorithm uses a dictionary to store numbers as it iterates through the list. For each number, it checks whether the complement (target − current number) already exists — if so, it returns their indices.'\n",
    "text=text.lower()\n",
    "text=re.sub(r'[^a-zA-Z\\s]','',text)\n",
    "words=word_tokenize(text)\n",
    "stop_words=set(stopwords.words('english'))\n",
    "words=[w for w in words if w not in stop_words]\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "cleaned_words=[lemmatizer.lemmatize(w) for w in words]\n",
    "clean_text=\" \".join(cleaned_words)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f308c7c3-376d-4d16-b73e-42f4009ff1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary without stopwords removed\n",
      "['amazing' 'is' 'love' 'nlp']\n",
      "vectors:\n",
      " [[0 0 1 1]\n",
      " [1 1 0 1]]\n",
      "CountVectorizer()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "texts=[\"I Love NLP\",\"NLP is amazing\"]\n",
    "\n",
    "Vectorizer_no_stop=CountVectorizer(stop_words=None)\n",
    "X_no_stop=Vectorizer_no_stop.fit_transform(texts)\n",
    "\n",
    "print(\"Vocabulary without stopwords removed\")\n",
    "print(Vectorizer_no_stop.get_feature_names_out())\n",
    "print(\"vectors:\\n\",X_no_stop.toarray())\n",
    "print(Vectorizer_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "541594a3-fe02-465d-9f1f-6a81424b5ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary with Stopwords Removed\n",
      "['amazing' 'love' 'nlp']\n",
      "vectors:\n",
      " [[0 1 1]\n",
      " [1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "texts=[\"I love NLP\",\"NLP is amazing\"]\n",
    "\n",
    "Vectorizer_no_stop=CountVectorizer(stop_words='english')\n",
    "X_no_stop=Vectorizer_no_stop.fit_transform(texts)\n",
    "\n",
    "print(\"Vocabulary with Stopwords Removed\")\n",
    "print(Vectorizer_no_stop.get_feature_names_out())\n",
    "print(\"vectors:\\n\",X_no_stop.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f7a2a-5fe2-4f8c-bc85-ebaf759a78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "+-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
